{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"../Data/train_lyrics_1000.csv\",header=0,encoding='utf-8')\n",
    "testing_data = pd.read_csv(\"../Data/valid_lyrics_200.csv\",header=0,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "      <th>mood</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAAAAW128F429D538.h5</td>\n",
       "      <td>Casual</td>\n",
       "      <td>I Didn't Mean To</td>\n",
       "      <td>Verse One:\\n\\nAlright I might\\nHave had a litt...</td>\n",
       "      <td>Hip Hop/Rap</td>\n",
       "      <td>sad</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAAAEF128F4273421.h5</td>\n",
       "      <td>Adam Ant</td>\n",
       "      <td>Something Girls</td>\n",
       "      <td>Adam Ant/Marco Pirroni\\nEvery girl is a someth...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>happy</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAAAFD128F92F423A.h5</td>\n",
       "      <td>Gob</td>\n",
       "      <td>Face the Ashes</td>\n",
       "      <td>I've just erased it's been a while, I've got a...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>sad</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAABJV128F1460C49.h5</td>\n",
       "      <td>Lionel Richie</td>\n",
       "      <td>Tonight Will Be Alright</td>\n",
       "      <td>Little darling \\nWhere you've been so long \\nI...</td>\n",
       "      <td>R&amp;B</td>\n",
       "      <td>happy</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAABLR128F423B7E3.h5</td>\n",
       "      <td>Blue Rodeo</td>\n",
       "      <td>Floating</td>\n",
       "      <td>Lead Vocal by Greg\\n\\nWell, these late night c...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>sad</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file         artist                    title  \\\n",
       "0  TRAAAAW128F429D538.h5         Casual         I Didn't Mean To   \n",
       "1  TRAAAEF128F4273421.h5       Adam Ant          Something Girls   \n",
       "2  TRAAAFD128F92F423A.h5            Gob           Face the Ashes   \n",
       "3  TRAABJV128F1460C49.h5  Lionel Richie  Tonight Will Be Alright   \n",
       "4  TRAABLR128F423B7E3.h5     Blue Rodeo                 Floating   \n",
       "\n",
       "                                              lyrics        genre   mood  year  \n",
       "0  Verse One:\\n\\nAlright I might\\nHave had a litt...  Hip Hop/Rap    sad  1994  \n",
       "1  Adam Ant/Marco Pirroni\\nEvery girl is a someth...         Rock  happy  1982  \n",
       "2  I've just erased it's been a while, I've got a...         Rock    sad  2007  \n",
       "3  Little darling \\nWhere you've been so long \\nI...          R&B  happy  1986  \n",
       "4  Lead Vocal by Greg\\n\\nWell, these late night c...         Rock    sad  1987  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into training and test data\n",
    "\n",
    "X_train = training_data['lyrics']\n",
    "y_train = testing_data['lyrics']\n",
    "X_test = training_data['mood']\n",
    "y_test = testing_data['mood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding the Target Variable\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create a label (category) encoder object\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the pandas column\n",
    "le.fit(X_test)\n",
    "le.fit(y_test)\n",
    "\n",
    "# View the labels (if you want)\n",
    "list(le.classes_)\n",
    "\n",
    "# Apply the fitted encoder to the pandas column\n",
    "X_test = le.transform(X_test) \n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words ['i', 'me', 'my', 'myself', 'we'] ...\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/stopwords_eng.txt', 'r') as infile:\n",
    "    stop_words = infile.read().splitlines()\n",
    "print('stop words %s ...' %stop_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Vectorizing Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing Using TFIDF Vectorizer\n",
    "\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "#TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "#MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "\n",
    "kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'stop_words':stop_words,\n",
    "            'binary':False,\n",
    "            'encoding':'utf-8',\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jjdksd', 'on', 'that', 'is', 'loko']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(porter_tokenizer('jjdksd on that is loko'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohitjain/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1567: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "x_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Vectorize validation texts.\n",
    "x_val_tfidf = vectorizer.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Parameters for the Model\n",
    "\n",
    "learning_rate=1e-3\n",
    "epochs=1000\n",
    "batch_size=128\n",
    "layers=2\n",
    "units=64\n",
    "dropout_rate=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_classes(labels):\n",
    "    \"\"\"Gets the total number of classes.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    # Returns\n",
    "        int, total number of classes.\n",
    "    # Raises\n",
    "        ValueError: if any label value in the range(0, num_classes - 1)\n",
    "            is missing or if number of classes is <= 1.\n",
    "    \"\"\"\n",
    "    num_classes = max(labels) + 1\n",
    "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
    "    if len(missing_classes):\n",
    "        raise ValueError('Missing samples with label value(s) '\n",
    "                         '{missing_classes}. Please make sure you have '\n",
    "                         'at least one sample for every label value '\n",
    "                         'in the range(0, {max_class})'.format(\n",
    "                            missing_classes=missing_classes,\n",
    "                            max_class=num_classes - 1))\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                         'Please make sure there are at least two classes '\n",
    "                         'of samples'.format(num_classes=num_classes))\n",
    "    return num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'sigmoid')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_last_layer_units_and_activation(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_units, op_activation = _get_last_layer_units_and_activation(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rohitjain/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/rohitjain/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Create model instance.\n",
    "\n",
    "model = mlp_model(layers=layers,\n",
    "                  units=units,\n",
    "                  dropout_rate=dropout_rate,\n",
    "                  input_shape=x_train_tfidf.shape[1:],\n",
    "                  num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with learning parameters.\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create callback for early stopping on validation loss. If the loss does not decrease in two consecutive tries, stop training.\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "WARNING:tensorflow:From /Users/rohitjain/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      " - 3s - loss: 0.6916 - acc: 0.5300 - val_loss: 0.6909 - val_acc: 0.4800\n",
      "Epoch 2/1000\n",
      " - 2s - loss: 0.6610 - acc: 0.6520 - val_loss: 0.6877 - val_acc: 0.4850\n",
      "Epoch 3/1000\n",
      " - 2s - loss: 0.6174 - acc: 0.8370 - val_loss: 0.6796 - val_acc: 0.5050\n",
      "Epoch 4/1000\n",
      " - 2s - loss: 0.5612 - acc: 0.9760 - val_loss: 0.6697 - val_acc: 0.5300\n",
      "Epoch 5/1000\n",
      " - 2s - loss: 0.4954 - acc: 0.9930 - val_loss: 0.6586 - val_acc: 0.5650\n",
      "Epoch 6/1000\n",
      " - 2s - loss: 0.4288 - acc: 0.9980 - val_loss: 0.6483 - val_acc: 0.5700\n",
      "Epoch 7/1000\n",
      " - 2s - loss: 0.3661 - acc: 0.9980 - val_loss: 0.6369 - val_acc: 0.6200\n",
      "Epoch 8/1000\n",
      " - 2s - loss: 0.3069 - acc: 0.9960 - val_loss: 0.6264 - val_acc: 0.6800\n",
      "Epoch 9/1000\n",
      " - 2s - loss: 0.2569 - acc: 0.9980 - val_loss: 0.6170 - val_acc: 0.6850\n",
      "Epoch 10/1000\n",
      " - 2s - loss: 0.2146 - acc: 0.9960 - val_loss: 0.6070 - val_acc: 0.6900\n",
      "Epoch 11/1000\n",
      " - 2s - loss: 0.1811 - acc: 0.9960 - val_loss: 0.5986 - val_acc: 0.7050\n",
      "Epoch 12/1000\n",
      " - 3s - loss: 0.1517 - acc: 0.9970 - val_loss: 0.5917 - val_acc: 0.7050\n",
      "Epoch 13/1000\n",
      " - 2s - loss: 0.1260 - acc: 0.9990 - val_loss: 0.5872 - val_acc: 0.7100\n",
      "Epoch 14/1000\n",
      " - 3s - loss: 0.1102 - acc: 0.9980 - val_loss: 0.5835 - val_acc: 0.7150\n",
      "Epoch 15/1000\n",
      " - 2s - loss: 0.0927 - acc: 0.9980 - val_loss: 0.5798 - val_acc: 0.7150\n",
      "Epoch 16/1000\n",
      " - 4s - loss: 0.0822 - acc: 0.9970 - val_loss: 0.5765 - val_acc: 0.7150\n",
      "Epoch 17/1000\n",
      " - 2s - loss: 0.0722 - acc: 0.9970 - val_loss: 0.5737 - val_acc: 0.7200\n",
      "Epoch 18/1000\n",
      " - 2s - loss: 0.0623 - acc: 0.9990 - val_loss: 0.5712 - val_acc: 0.7150\n",
      "Epoch 19/1000\n",
      " - 2s - loss: 0.0568 - acc: 0.9970 - val_loss: 0.5686 - val_acc: 0.7200\n",
      "Epoch 20/1000\n",
      " - 2s - loss: 0.0500 - acc: 0.9960 - val_loss: 0.5671 - val_acc: 0.7300\n",
      "Epoch 21/1000\n",
      " - 2s - loss: 0.0449 - acc: 0.9970 - val_loss: 0.5657 - val_acc: 0.7300\n",
      "Epoch 22/1000\n",
      " - 2s - loss: 0.0408 - acc: 0.9990 - val_loss: 0.5652 - val_acc: 0.7350\n",
      "Epoch 23/1000\n",
      " - 2s - loss: 0.0393 - acc: 0.9960 - val_loss: 0.5654 - val_acc: 0.7350\n",
      "Epoch 24/1000\n",
      " - 2s - loss: 0.0353 - acc: 0.9970 - val_loss: 0.5644 - val_acc: 0.7350\n",
      "Epoch 25/1000\n",
      " - 2s - loss: 0.0317 - acc: 0.9970 - val_loss: 0.5632 - val_acc: 0.7300\n",
      "Epoch 26/1000\n",
      " - 2s - loss: 0.0295 - acc: 0.9980 - val_loss: 0.5630 - val_acc: 0.7350\n",
      "Epoch 27/1000\n",
      " - 2s - loss: 0.0280 - acc: 0.9990 - val_loss: 0.5626 - val_acc: 0.7400\n",
      "Epoch 28/1000\n",
      " - 2s - loss: 0.0250 - acc: 0.9980 - val_loss: 0.5628 - val_acc: 0.7350\n",
      "Epoch 29/1000\n",
      " - 2s - loss: 0.0233 - acc: 0.9980 - val_loss: 0.5632 - val_acc: 0.7350\n"
     ]
    }
   ],
   "source": [
    "# Train and validate model.\n",
    "history = model.fit(\n",
    "            x_train_tfidf,\n",
    "            X_test,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val_tfidf, y_test),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7350000143051147, loss: 0.563155632019043\n"
     ]
    }
   ],
   "source": [
    "# Print results.\n",
    "history = history.history\n",
    "print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'Without you, I feel broke Like Im half of a whole Without you, Ive got no hand to hold Without you, I feel torn Like a sail in a storm Without you, Im just a sad song Im just a sad song'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_token=porter_tokenizer(x)\n",
    "x_input = vectorizer.transform([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_input = selector.transform(x_input).astype('float32')\n",
    "\n",
    "predictions = model.predict(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 81233)\t0.43776571751094345\n",
      "  (0, 80615)\t0.12663762343763518\n",
      "  (0, 74701)\t0.17355939793195005\n",
      "  (0, 68979)\t0.1500728907353273\n",
      "  (0, 66441)\t0.250859917962649\n",
      "  (0, 60736)\t0.17634370196165494\n",
      "  (0, 60458)\t0.32116958845060173\n",
      "  (0, 40229)\t0.1209677318943262\n",
      "  (0, 35700)\t0.21791986100660443\n",
      "  (0, 35699)\t0.21791986100660443\n",
      "  (0, 34791)\t0.47656404134585845\n",
      "  (0, 33323)\t0.09987826363699612\n",
      "  (0, 30891)\t0.2308944647132536\n",
      "  (0, 30866)\t0.11758620944781517\n",
      "  (0, 30797)\t0.1500728907353273\n",
      "  (0, 29453)\t0.2308944647132536\n",
      "  (0, 29284)\t0.06770225770289866\n",
      "  (0, 23005)\t0.1615491386047857\n",
      "  (0, 8419)\t0.1488178549749624\n"
     ]
    }
   ],
   "source": [
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rounded = [np.round(x) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n"
     ]
    }
   ],
   "source": [
    "if rounded[0] == 1:\n",
    "    print('happy')\n",
    "else:\n",
    "    print('sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_converter(string):\n",
    "    x = string\n",
    "    x_input = vectorizer.transform([x])\n",
    "    print(x_input)\n",
    "    predictions = model.predict(x_input)\n",
    "    rounded = [np.round(x) for x in predictions]\n",
    "    if rounded[0] == 1:\n",
    "        out = 'sad'\n",
    "    else:\n",
    "        out = 'happy'\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 49760)\t0.2294992869652013\n",
      "  (0, 49743)\t0.5038558268933042\n",
      "  (0, 47075)\t0.2538883630961472\n",
      "  (0, 29840)\t0.17467441170297782\n",
      "  (0, 25772)\t0.2538883630961472\n",
      "  (0, 25756)\t0.4792433362646209\n",
      "  (0, 25733)\t0.5516841992471969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_converter('I re ihe sad joy happy happy sad sad happy happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 49760)\t0.5342634078961679\n",
      "  (0, 49743)\t0.7819682426149922\n",
      "  (0, 25733)\t0.32107363411652745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sad'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_converter('sad sad happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ngram_range': (1, 2), 'dtype': 'int32', 'strip_accents': 'unicode', 'decode_error': 'replace', 'analyzer': 'word', 'stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'], 'binary': False, 'encoding': 'utf-8'}\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/rohitjain/Documents/MLPresentation/song-mood/model/vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer,file)\n",
    "with open('/Users/rohitjain/Documents/MLPresentation/song-mood/model/kwargs.pkl', 'wb') as file:\n",
    "    pickle.dump(kwargs,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"/Users/rohitjain/Documents/MLPresentation/song-mood/model/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"/Users/rohitjain/Documents/MLPresentation/song-mood/model/model_weights.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
